# Data input pathways

The **overall aim** of this section is to describe the general path that
data takes through Seedcase, from input into the final output.
Specifically, these items are described as:

-   *Input*: Because we currently focus on health research, the type of
    input data and metadata will be what is typically generated from
    health studies.
-   *Output*: The final output object would be the input data stored
    together as a single database, or at least multiple databases and
    files explicitly linked in such a way that it conceptually
    represents a single database. In data engineering terms, this would
    be a well-connected ["data
    warehouse"](https://en.wikipedia.org/wiki/Data_warehouse) that will
    be usable for other components and paths (described in other
    sections).
-   *Path*: The computational and programmatic steps the input data and
    metadata takes from being uploaded by a human (and potentially
    automatically by a program) into Seedcase, passing through quality
    control checks, adding to the changelog, and storage into the
    database.

This section doesn't go over what is already covered by other sections,
such as are more detail on the backend database/data warehouse, the
frontend, or the API. It also doesn't go over security details.

<!-- TODO: Include links to these sections once they are done. -->

## Expected type of input data

Given the (current) focus on health data as well as our own experiences,
we make some assumptions about the type of data that will be input into
Seedcase. Health data tends to consist of specific types of data:

-   **Clinical**: This data is typically collected during patient visits
    to doctors. Depending on the country or adminitrative region, there
    will likely already be well-established data processing and storage
    pipelines in place.
-   **Register**: This type of data is highly dependent on the country
    or region. Generally, this data is collected for national or
    regional administrative purposes, such as, recording employment
    status, income, address, medication purchases, and diagnoses. Like
    the routine clinical data, the pipelines in place for processing and
    storage of this data are usually very extensive and established.
-   **Biological sample analysis**: This type of data is generated from
    biological samples, like blood, saliva, semen, hair, or urine. Data
    generated from sample analytic techniques often produce large
    volumes of data per person. Samples may be generated in larger
    established laboratories or in smaller research groups, depending on
    how what analytic technology is used and how new it is. The
    structure and format of the generated data also tends to be highly
    variable and depends heavily on the technology used, sometimes
    requiring specialized software to process and output.
-   **Survey or questionnaire**: This type of data is often done based
    on a given study's aims and research questions. There are hundreds
    of different questionnaires that can have highly specific purposes
    and uses for their data. They are also highly variable in the volume
    of data collected based on the survey, and on the format of the
    data.

<!-- TODO: Others? -->

## Expected flow of input data

The above described data tends to fit into, mostly, two categories for
data input.

-   *Routine or continuous collection*, where ingested data into
    Seedcase would occur as soon as the data was collected from one
    "observational unit"[^data-input-1] or very shortly afterwards.
    Clinical data as well as survey or questionnaire data may likely
    fall under this category.
-   *Batch collection*, where ingested data occurs some time after the
    data was collected and from multiple observational units. Biological
    sample data would fall under this category, since laboratories
    usually run several samples at once and input data after internal
    quality control checks and machine-specific data processing. While
    register-based data does get collected continuously, direct access
    to it is only given on a batch basis, usually once every year.
    Survey data may also come in batches, depending on the questionnaire
    and software used for its collection.

[^data-input-1]: Observational unit is the "entity" that the data was
    collected from at a given point in time, such as a human participant
    in a cohort study or a rat in an animal study at a specific time
    point.

For sources of data from routine collection with well-established data
input processes, the data input pipeline would likely involve
redirecting these data sources from their generation into Seedcase via a
direct call to the API so the data continues on to the backend and
eventual data storage.

Sources of data that don't have well-established data input processes,
such as from hospitals or medical laboratories, would need to use the
Seedcase data batch-input portal. This portal would only accept data
that is in a pre-defined format (as determined and created by the data
owner) and would include documentation, and potentially automation
scripts, on how to pre-process the data prior to uploading it.

Once the data is submitted through the portal, it would get sent in an
encrypted, legally-compliant format to the server and stored in the way
defined by the API and common data model.

## Pre- and post-processing of data

Any automated processing that is developed specific to a project would
need to adhere to the API's conventions. If any issues are found or if
the data is entirely new to the database, they get sent to a log and
User 4 would receive a notification to deal with the issue.

## Data, metadata, and expected values input

### Quality control

Any new or updated data that is uploaded would trigger generic automated
data cleaning, processing, quality control checks of this new data.

### Versioning (internal vs external)

-   versioning, external via DOI with releases of summary stats and
    changelog. (how often?)

If there are no issues or the issues have been dealt with, an automated
script would take a snapshot of the data with the VC system, the version
number (based on [Semantic Versioning](https://semver.org/)) of the data
would be updated, an entry would get added to the changelog, and the
formal database would get updated.

## From input to storage

TODO: Directed graph diagram showing pathways actions take from data
input into final database
