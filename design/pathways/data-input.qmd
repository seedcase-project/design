# Data input pathways

<!-- TODO: Others? -->

## Pre- and post-processing of data

Any automated processing that is developed specific to a project would
need to adhere to the API's conventions. If any issues are found or if
the data is entirely new to the database, they get sent to a log and
User 4 would receive a notification to deal with the issue.

## Data, metadata, and expected values input

### Quality control

Any new or updated data that is uploaded would trigger generic automated
data cleaning, processing, quality control checks of this new data.

qc could be based on basic things like checking if file is correct,
contains proper delims, etc.

<!-- TODO: Would there be a QC for streaming data? -->

### Versioning (internal vs external)

-   versioning, external via DOI with releases of summary stats and
    changelog. (how often?)

If there are no issues or the issues have been dealt with, an automated
script would take a snapshot of the data with the VC system, the version
number (based on [Semantic Versioning](https://semver.org/)) of the data
would be updated, an entry would get added to the changelog, and the
formal database would get updated.

## From input to storage

### Flow for "sample analysis" data

```{mermaid}
%%| eval: true
%%| label: fig-data-input-sample-analysis
%%| fig-cap: "The flow of sample analysis-type data through Seedcase."
%%| file: images/data-input-sample-analysis.mmd
```

### Flow for batch data

```{mermaid}
%%| eval: true
%%| label: fig-data-input-batch
%%| fig-cap: "The flow of batch collection data through Seedcase."
%%| file: images/data-input-batch.mmd
```
